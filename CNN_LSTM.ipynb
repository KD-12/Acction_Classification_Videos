{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Recognition @ UCF101  \n",
    "**Due date: 11:59 pm on Dec. 11, 2018 (Tuesday)**\n",
    "\n",
    "## Description\n",
    "---\n",
    "In this homework, you will be doing action recognition using Recurrent Neural Network (RNN), (Long-Short Term Memory) LSTM in particular. You will be given a dataset called UCF101, which consists of 101 different actions/classes and for each action, there will be 145 samples. We tagged each sample into either training or testing. Each sample is supposed to be a short video, but we sampled 25 frames from each videos to reduce the data amount. Consequently, a training sample is a tuple of 3D volume with one dimension encoding *temporal correlation* between frames and a label indicating what action it is.\n",
    "\n",
    "To tackle this problem, we aim to build a neural network that can not only capture spatial information of each frame but also temporal information between frames. Fortunately, you don't have to do this on your own. RNN — a type of neural network designed to deal with time-series data — is right here for you to use. In particular, you will be using LSTM for this task.\n",
    "\n",
    "Instead of training a end-to-end neural network from scratch whose computation is prohibitively expensive for CPUs. We divide this into two steps: feature extraction and modelling. Below are the things you need to implement for this homework:\n",
    "- **{35 pts} Feature extraction**. Use the pretrained VGG network to extract features from each frame. Specifically, we recommend  to use the activations of the first fully connected layer `torchvision.models.vgg16` (4096 dim) as features of each video frame. This will result into a 4096x25 matrix for each video. \n",
    "    **hints**: \n",
    "    - use `scipy.io.savemat()` to save feature to '.mat' file and `scipy.io.loadmat()` load feature.\n",
    "    - norm your images using `torchvision.transforms`\n",
    "    ```\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    prep = transforms.Compose([ transforms.ToTensor(), normalize ])\n",
    "    prep(img)\n",
    "    \n",
    "    ```\n",
    "    More detils of image preprocessing in PyTorch can be found at http://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "    \n",
    "- **{35 pts} Modelling**. With the extracted features, build an LSTM network which takes a 4096x25 sample as input, and outputs the action label of that sample.\n",
    "- **{20 pts} Evaluation**. After training your network, you need to evaluate your model with the testing data by computing the prediction accuracy. Moreover, you need to compare the result of your network with that of support vector machine (SVM) (stacking the 4096x25 feature matrix to a long vector and train a SVM).\n",
    "- **{10 pts} Report**. Details regarding the report can be found in the submission section below.\n",
    "\n",
    "Notice that the size of the raw images is 256x340, whereas VGG16 takes 224x224 images as inputs. To solve this problem, instead of resizing the images which unfavorably changes the spatial ratio, we take a better solution: Cropping five 224x224 images at the image center and four corners and compute the 4096-dim VGG16 features for each of them, and average these five 4096-dim feature to get final feature representation for the raw image.\n",
    "\n",
    "In order to save you computational time, we did the feature extraction of most samples for you except for class 1. For class 1, we provide you with the raw images, and you need to write code to extract the feature of the samples in class 1. Instead of training over the whole dataset on CPUs which mays cost you serval days, **use the first 15** classes of the whole dataset. The same applies to those who have access to GPUs.\n",
    "\n",
    "\n",
    "## Dataset\n",
    "Download dataset at [UCF101](http://vision.cs.stonybrook.edu/~yangwang/public/UCF101_dimitris_course.zip). \n",
    "\n",
    "The dataset is consist of the following two parts: video images and extracted features.\n",
    "\n",
    "### 1. Video Images  \n",
    "\n",
    "UCF101 dataset contains 101 actions and 13,320 videos in total.  \n",
    "\n",
    "+ `annos/actions.txt`  \n",
    "  + lists all the actions (`ApplyEyeMakeup`, .., `YoYo`)   \n",
    "  \n",
    "+ `annots/videos_labels_subsets.txt`  \n",
    "  + lists all the videos (`v_000001`, .., `v_013320`)  \n",
    "  + labels (`1`, .., `101`)  \n",
    "  + subsets (`1` for train, `2` for test)  \n",
    "\n",
    "+ `images_class1/`  \n",
    "  + contains videos belonging to class 1 (`ApplyEyeMakeup`)  \n",
    "  + each video folder contains 25 frames  \n",
    "\n",
    "\n",
    "### 2. Video Features\n",
    "\n",
    "+ `extract_vgg16_relu6.py`  \n",
    "  + used to extract video features  \n",
    "     + Given an image (size: 256x340), we get 5 crops (size: 224x224) at the image center and four corners. The `vgg16-relu6` features are extracted for all 5 crops and subsequently averaged to form a single feature vector (size: 4096).  \n",
    "     + Given a video, we process its 25 images seuqentially. In the end, each video is represented as a feature sequence (size: 4096 x 25).  \n",
    "  + written in PyTorch; supports both CPU and GPU.  \n",
    "\n",
    "+ `vgg16_relu6/`  \n",
    "   + contains all the video features, EXCEPT those belonging to class 1 (`ApplyEyeMakeup`)  \n",
    "   + you need to run script `extract_vgg16_relu6.py` to complete the feature extracting process   \n",
    "\n",
    "\n",
    "## Some Tutorials\n",
    "- Good materials for understanding RNN and LSTM\n",
    "    - http://blog.echen.me\n",
    "    - http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "    - http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- Implementing RNN and LSTM with PyTorch\n",
    "    - [LSTM with PyTorch](http://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#sphx-glr-beginner-nlp-sequence-models-tutorial-py)\n",
    "    - [RNN with PyTorch](http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your codes here\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, RMSprop\n",
    "from torch.utils import data\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torchvision.models.inception as inception\n",
    "\n",
    "import scipy\n",
    "from scipy import io\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./UCF101_dimitris_course/UCF101_release/vgg16_relu6/\"\n",
    "train_data = []\n",
    "test_data = []\n",
    "train_label = []\n",
    "test_label = []\n",
    "with open(\"./UCF101_dimitris_course/UCF101_release/annos/videos_labels_subsets.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line_list = line.rstrip().split(\"\\t\")\n",
    "        if line_list[1] == \"16\":\n",
    "            break\n",
    "        if line_list[-1] == \"1\":\n",
    "            \n",
    "            train_data.append(scipy.io.loadmat(path+line_list[0]+\".mat\")[\"Feature\"])\n",
    "            train_label.append(int(line_list[1]))\n",
    "                              \n",
    "        else:\n",
    "            test_data.append(scipy.io.loadmat(path+line_list[0]+\".mat\")[\"Feature\"])\n",
    "            test_label.append(int(line_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data, train_label, test_size=0.10, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(X_train, y_train)\n",
    "validation_dataset = MyDataset(X_val, y_val)\n",
    "test_dataset = MyDataset(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = 25, shuffle = True, drop_last=True)\n",
    "val_dataset_loader = torch.utils.data.DataLoader(dataset = validation_dataset, batch_size = 25, shuffle = True, drop_last=True)\n",
    "test_dataset_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = 25, shuffle = True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#following model gives accuracy of 80.63 with 21 epochs\n",
    "class LSTM_Classify(nn.Module):\n",
    "    def __init__(self,feature_size, output_size):\n",
    "        super(LSTM_Classify, self).__init__()\n",
    "        \n",
    "        self.recurrent_layer = nn.LSTM(feature_size, 25, 2)\n",
    "        self.prediction_layer = nn.Linear(25, output_size)\n",
    "        \n",
    "    def forward(self, input, h_t_1=None, c_t_1=None):\n",
    "        rnn_output, (hn, cn) = self.recurrent_layer(input)\n",
    "        prediction_output = self.prediction_layer(rnn_output[:,-1])\n",
    "        return prediction_output\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "#following model was run on GPU getting GPU memory full\n",
    "\n",
    "class LSTM_Classify(nn.Module):\n",
    "    def __init__(self,feature_size, output_size):\n",
    "        super(LSTM_Classify, self).__init__()\n",
    "        self.inception = nn.Sequential(\n",
    "                                       nn.Conv2d(1, 15, stride = 1, kernel_size = 2),\n",
    "                                       nn.LeakyReLU(),\n",
    "                                       nn.BatchNorm2d(15),\n",
    "                                       inception.InceptionB(15),\n",
    "                                       nn.MaxPool2d(2, stride = 2),\n",
    "                                       nn.Conv2d(495, 1, stride = 1, kernel_size = 2),\n",
    "                                       nn.MaxPool2d(2, stride = 2),\n",
    "                                       nn.LeakyReLU(),\n",
    "                                       nn.BatchNorm2d(1))\n",
    "\n",
    "        self.linear = nn.Linear(1022, 500)\n",
    "        self.recurrent_layer = nn.LSTM(500, 25, 2)\n",
    "        self.prediction_layer = nn.Linear(25, output_size)\n",
    "    def forward(self, input, h_t_1=None, c_t_1=None):\n",
    "        inception_output = self.inception(input.view((25, 1, 25, 4096)))\n",
    "        N, C, H, W = inception_output.size()\n",
    "        linear_input = inception_output.view(N, -1)\n",
    "        linear_output = self.linear(linear_input)\n",
    "        recurrent_input = linear_output.view(25, 1, 500)\n",
    "        rnn_output, (hn, cn) = self.recurrent_layer(recurrent_input)\n",
    "        prediction_output = self.prediction_layer(rnn_output[:,-1])\n",
    "        #return inception_output\n",
    "        return prediction_output\n",
    "\"\"\"\n",
    "class LSTM_Classify(nn.Module):\n",
    "    def __init__(self,feature_size, output_size):\n",
    "        super(LSTM_Classify, self).__init__()\n",
    "        self.inception = nn.Sequential(\n",
    "                                       nn.Conv2d(1, 15, stride = 1, kernel_size = 2),\n",
    "                                       nn.MaxPool2d(2, stride = 2),\n",
    "                                       nn.LeakyReLU(),\n",
    "                                       nn.BatchNorm2d(15),\n",
    "                                       nn.Conv2d(15, 1, stride = 1, kernel_size = 2),\n",
    "                                       nn.MaxPool2d(2, stride = 2),\n",
    "                                       nn.LeakyReLU(),\n",
    "                                       nn.BatchNorm2d(1))\n",
    "                                       #nn.Conv2d(1, 15, stride = 1, kernel_size = 2))\n",
    "                                       \n",
    "        self.linear = nn.Linear(5115, 500)\n",
    "        self.recurrent_layer = nn.LSTM(500, 25, 2)\n",
    "        self.prediction_layer = nn.Linear(25, output_size)\n",
    "        \n",
    "    def forward(self, input, h_t_1=None, c_t_1=None):\n",
    "        inception_output = self.inception(input.view((25, 1, 25, 4096)))\n",
    "        N, C, H, W = inception_output.size()\n",
    "        linear_input = inception_output.view(N, -1)\n",
    "        linear_output = self.linear(linear_input)\n",
    "        recurrent_input = linear_output.view(25, 1, 500)\n",
    "        rnn_output, (hn, cn) = self.recurrent_layer(recurrent_input)\n",
    "        prediction_output = self.prediction_layer(rnn_output[:,-1])\n",
    "        return prediction_output\n",
    "        #return linear_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 50\n",
    "feature_size = 4096\n",
    "output_size  = 16\n",
    "model = LSTM_Classify(feature_size, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.SGD(model.parameters(), lr = 1e-2)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr = 1e-1)\n",
    "loss_function = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrote this to find precision point to stop at correct epoch.\n",
    "def validate(model, loss_function, optimizer, dataloader,epoch):\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    model.eval()\n",
    "    for step_num, tup in enumerate(dataloader):\n",
    "\n",
    "        x_var = Variable(tup[0])\n",
    "        y_var = Variable(tup[1].long())\n",
    "        output_scores = model(x_var)\n",
    "        loss = loss_function(output_scores, y_var)\n",
    "        \n",
    "        val_loss += loss.cpu().data.item() #* tup[0].size(0)\n",
    "        _, prediction = torch.max(output_scores.data, 1)\n",
    "\n",
    "        val_acc += torch.sum(prediction == tup[1])\n",
    "    val_acc_epoch = (float(val_acc)/(len(dataloader)*25))*100\n",
    "    val_loss_epoch = (float(val_loss/(len(dataloader)*25)))\n",
    "    print(\"Epoch {}, Validation Accuracy: {}, Validation Loss: {}\".format(epoch+1, val_acc_epoch, val_loss_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_function, optimizer, train_dataloader,val_dataloader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting epoch number %d' % (epoch + 1))\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        model.train()\n",
    "        for step_num, tup in enumerate(train_dataloader):\n",
    "         \n",
    "            x_var = Variable(tup[0])\n",
    "           \n",
    "            y_var = Variable(tup[1].long())\n",
    "            optimizer.zero_grad()\n",
    "            output_scores = model(x_var)\n",
    "            #print(output_scores.shape)\n",
    "            loss = loss_function(output_scores, y_var)             \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.cpu().data.item() #* tup[0].size(0)\n",
    "            _, prediction = torch.max(output_scores.data, 1)\n",
    "            \n",
    "            train_acc += torch.sum(prediction == tup[1])\n",
    "        train_acc_epoch = (float(train_acc)/(len(train_dataloader)*25))*100\n",
    "        train_loss_epoch = (float(train_loss/(len(train_dataloader)*25)))\n",
    "        print(\"Epoch {}, Train Accuracy: {}, Train Loss: {}\".format(epoch+1, train_acc_epoch, train_loss_epoch))\n",
    "        if (step_num) % 1 == 0:\n",
    "                validate(model, loss_function, optimizer, val_dataloader,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch number 1\n",
      "Epoch 1, Train Accuracy: 13.254901960784313, Train Loss: 0.10828352535472197\n",
      "Epoch 1, Validation Accuracy: 22.400000000000002, Validation Loss: 0.10709198379516602\n",
      "Starting epoch number 2\n",
      "Epoch 2, Train Accuracy: 32.15686274509804, Train Loss: 0.10376270275489957\n",
      "Epoch 2, Validation Accuracy: 34.4, Validation Loss: 0.10250077056884765\n",
      "Starting epoch number 3\n",
      "Epoch 3, Train Accuracy: 48.392156862745104, Train Loss: 0.09975511663100299\n",
      "Epoch 3, Validation Accuracy: 54.400000000000006, Validation Loss: 0.09840564155578613\n",
      "Starting epoch number 4\n",
      "Epoch 4, Train Accuracy: 61.01960784313726, Train Loss: 0.09540253770117667\n",
      "Epoch 4, Validation Accuracy: 64.8, Validation Loss: 0.09392206954956055\n",
      "Starting epoch number 5\n",
      "Epoch 5, Train Accuracy: 69.25490196078431, Train Loss: 0.0906360964681588\n",
      "Epoch 5, Validation Accuracy: 68.8, Validation Loss: 0.08891266250610351\n",
      "Starting epoch number 6\n",
      "Epoch 6, Train Accuracy: 76.86274509803923, Train Loss: 0.0855659948610792\n",
      "Epoch 6, Validation Accuracy: 77.60000000000001, Validation Loss: 0.08363041114807129\n",
      "Starting epoch number 7\n",
      "Epoch 7, Train Accuracy: 83.2156862745098, Train Loss: 0.07967214107513428\n",
      "Epoch 7, Validation Accuracy: 82.39999999999999, Validation Loss: 0.07702902889251709\n",
      "Starting epoch number 8\n",
      "Epoch 8, Train Accuracy: 88.7843137254902, Train Loss: 0.07321788647595574\n",
      "Epoch 8, Validation Accuracy: 88.8, Validation Loss: 0.07058964729309082\n",
      "Starting epoch number 9\n",
      "Epoch 9, Train Accuracy: 92.31372549019608, Train Loss: 0.0670010976230397\n",
      "Epoch 9, Validation Accuracy: 91.2, Validation Loss: 0.06355340766906738\n",
      "Starting epoch number 10\n",
      "Epoch 10, Train Accuracy: 95.37254901960785, Train Loss: 0.06068037678213681\n",
      "Epoch 10, Validation Accuracy: 92.80000000000001, Validation Loss: 0.05733485317230225\n",
      "Starting epoch number 11\n",
      "Epoch 11, Train Accuracy: 96.78431372549021, Train Loss: 0.05420543025521671\n",
      "Epoch 11, Validation Accuracy: 94.39999999999999, Validation Loss: 0.052031653404235836\n",
      "Starting epoch number 12\n",
      "Epoch 12, Train Accuracy: 97.25490196078431, Train Loss: 0.048182370896432916\n",
      "Epoch 12, Validation Accuracy: 97.6, Validation Loss: 0.04550251293182373\n",
      "Starting epoch number 13\n",
      "Epoch 13, Train Accuracy: 98.66666666666667, Train Loss: 0.04281612335466871\n",
      "Epoch 13, Validation Accuracy: 97.6, Validation Loss: 0.04143223476409912\n",
      "Starting epoch number 14\n",
      "Epoch 14, Train Accuracy: 98.98039215686273, Train Loss: 0.03792076550278009\n",
      "Epoch 14, Validation Accuracy: 99.2, Validation Loss: 0.035803127765655515\n",
      "Starting epoch number 15\n",
      "Epoch 15, Train Accuracy: 99.6078431372549, Train Loss: 0.03334650591307995\n",
      "Epoch 15, Validation Accuracy: 100.0, Validation Loss: 0.03196183156967163\n",
      "Starting epoch number 16\n",
      "Epoch 16, Train Accuracy: 100.0, Train Loss: 0.029465130777919995\n",
      "Epoch 16, Validation Accuracy: 100.0, Validation Loss: 0.02837658166885376\n",
      "Starting epoch number 17\n",
      "Epoch 17, Train Accuracy: 100.0, Train Loss: 0.025888749758402507\n",
      "Epoch 17, Validation Accuracy: 100.0, Validation Loss: 0.02489041519165039\n",
      "Starting epoch number 18\n",
      "Epoch 18, Train Accuracy: 100.0, Train Loss: 0.023056506175620883\n",
      "Epoch 18, Validation Accuracy: 100.0, Validation Loss: 0.021927382469177244\n",
      "Starting epoch number 19\n",
      "Epoch 19, Train Accuracy: 100.0, Train Loss: 0.020146061392391428\n",
      "Epoch 19, Validation Accuracy: 100.0, Validation Loss: 0.01966340184211731\n",
      "Starting epoch number 20\n",
      "Epoch 20, Train Accuracy: 100.0, Train Loss: 0.01791782865337297\n",
      "Epoch 20, Validation Accuracy: 100.0, Validation Loss: 0.0176937575340271\n",
      "Starting epoch number 21\n",
      "Epoch 21, Train Accuracy: 100.0, Train Loss: 0.015605247979070626\n",
      "Epoch 21, Validation Accuracy: 100.0, Validation Loss: 0.01522091507911682\n",
      "Starting epoch number 22\n",
      "Epoch 22, Train Accuracy: 100.0, Train Loss: 0.013846645027983422\n",
      "Epoch 22, Validation Accuracy: 100.0, Validation Loss: 0.013401135921478272\n",
      "Starting epoch number 23\n",
      "Epoch 23, Train Accuracy: 100.0, Train Loss: 0.012270400080026365\n",
      "Epoch 23, Validation Accuracy: 100.0, Validation Loss: 0.012154639959335327\n",
      "Starting epoch number 24\n",
      "Epoch 24, Train Accuracy: 100.0, Train Loss: 0.010830257686914183\n",
      "Epoch 24, Validation Accuracy: 100.0, Validation Loss: 0.011014452457427978\n",
      "Starting epoch number 25\n",
      "Epoch 25, Train Accuracy: 100.0, Train Loss: 0.009552651994368609\n",
      "Epoch 25, Validation Accuracy: 100.0, Validation Loss: 0.009848660945892334\n",
      "Starting epoch number 26\n",
      "Epoch 26, Train Accuracy: 100.0, Train Loss: 0.008480291834064558\n",
      "Epoch 26, Validation Accuracy: 100.0, Validation Loss: 0.008696743488311768\n",
      "Starting epoch number 27\n",
      "Epoch 27, Train Accuracy: 100.0, Train Loss: 0.007557660446447485\n",
      "Epoch 27, Validation Accuracy: 100.0, Validation Loss: 0.0077482017278671265\n",
      "Starting epoch number 28\n",
      "Epoch 28, Train Accuracy: 100.0, Train Loss: 0.006717030581306009\n",
      "Epoch 28, Validation Accuracy: 100.0, Validation Loss: 0.006981321096420288\n",
      "Starting epoch number 29\n",
      "Epoch 29, Train Accuracy: 100.0, Train Loss: 0.005933185430134044\n",
      "Epoch 29, Validation Accuracy: 99.2, Validation Loss: 0.006381758689880371\n",
      "Starting epoch number 30\n",
      "Epoch 30, Train Accuracy: 100.0, Train Loss: 0.005245035232282152\n",
      "Epoch 30, Validation Accuracy: 100.0, Validation Loss: 0.005526778936386109\n",
      "Starting epoch number 31\n",
      "Epoch 31, Train Accuracy: 100.0, Train Loss: 0.004726981508965586\n",
      "Epoch 31, Validation Accuracy: 100.0, Validation Loss: 0.00481721967458725\n",
      "Starting epoch number 32\n",
      "Epoch 32, Train Accuracy: 100.0, Train Loss: 0.004209874091195125\n",
      "Epoch 32, Validation Accuracy: 100.0, Validation Loss: 0.0045223100781440735\n",
      "Starting epoch number 33\n",
      "Epoch 33, Train Accuracy: 100.0, Train Loss: 0.0037948400892463385\n",
      "Epoch 33, Validation Accuracy: 100.0, Validation Loss: 0.0040656706094741825\n",
      "Starting epoch number 34\n",
      "Epoch 34, Train Accuracy: 100.0, Train Loss: 0.0034327549794140985\n",
      "Epoch 34, Validation Accuracy: 99.2, Validation Loss: 0.0038735496401786805\n",
      "Starting epoch number 35\n",
      "Epoch 35, Train Accuracy: 100.0, Train Loss: 0.003116563482611787\n",
      "Epoch 35, Validation Accuracy: 100.0, Validation Loss: 0.0036939498782157898\n",
      "Starting epoch number 36\n",
      "Epoch 36, Train Accuracy: 100.0, Train Loss: 0.0027950718882037142\n",
      "Epoch 36, Validation Accuracy: 100.0, Validation Loss: 0.0030471100211143494\n",
      "Starting epoch number 37\n",
      "Epoch 37, Train Accuracy: 100.0, Train Loss: 0.0025529821480021757\n",
      "Epoch 37, Validation Accuracy: 100.0, Validation Loss: 0.0028893228471279144\n",
      "Starting epoch number 38\n",
      "Epoch 38, Train Accuracy: 100.0, Train Loss: 0.0023378959064390145\n",
      "Epoch 38, Validation Accuracy: 100.0, Validation Loss: 0.0026526270508766174\n",
      "Starting epoch number 39\n",
      "Epoch 39, Train Accuracy: 100.0, Train Loss: 0.002112977417076335\n",
      "Epoch 39, Validation Accuracy: 100.0, Validation Loss: 0.002491357356309891\n",
      "Starting epoch number 40\n",
      "Epoch 40, Train Accuracy: 100.0, Train Loss: 0.0019340435985256645\n",
      "Epoch 40, Validation Accuracy: 100.0, Validation Loss: 0.002316464841365814\n",
      "Starting epoch number 41\n",
      "Epoch 41, Train Accuracy: 100.0, Train Loss: 0.001789215341502545\n",
      "Epoch 41, Validation Accuracy: 100.0, Validation Loss: 0.002185128450393677\n",
      "Starting epoch number 42\n",
      "Epoch 42, Train Accuracy: 100.0, Train Loss: 0.0016562535657602198\n",
      "Epoch 42, Validation Accuracy: 100.0, Validation Loss: 0.0017874149978160858\n",
      "Starting epoch number 43\n",
      "Epoch 43, Train Accuracy: 100.0, Train Loss: 0.0015214875282025804\n",
      "Epoch 43, Validation Accuracy: 100.0, Validation Loss: 0.0017543592751026154\n",
      "Starting epoch number 44\n",
      "Epoch 44, Train Accuracy: 100.0, Train Loss: 0.0014238745573104596\n",
      "Epoch 44, Validation Accuracy: 100.0, Validation Loss: 0.0017688947319984437\n",
      "Starting epoch number 45\n",
      "Epoch 45, Train Accuracy: 100.0, Train Loss: 0.0013297049803476708\n",
      "Epoch 45, Validation Accuracy: 100.0, Validation Loss: 0.0017882783412933349\n",
      "Starting epoch number 46\n",
      "Epoch 46, Train Accuracy: 100.0, Train Loss: 0.0012364955947679632\n",
      "Epoch 46, Validation Accuracy: 100.0, Validation Loss: 0.0014520129412412643\n",
      "Starting epoch number 47\n",
      "Epoch 47, Train Accuracy: 100.0, Train Loss: 0.0011685100460753722\n",
      "Epoch 47, Validation Accuracy: 100.0, Validation Loss: 0.0013107653856277465\n"
     ]
    }
   ],
   "source": [
    "train(model, loss_function, optimizer, train_dataset_loader,val_dataset_loader, 47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, testloader):\n",
    "    model.eval()\n",
    "    test_acc = 0.0\n",
    "    for step_num, tup in enumerate(testloader):\n",
    "        test_output = model(tup[0])\n",
    "        _, prediction = torch.max(test_output.data, 1)\n",
    "        test_acc += torch.sum(prediction == tup[1])\n",
    "        \n",
    "    test_acc = (float(test_acc) / (len(testloader)*25))*100\n",
    "    print(\"Got Test Accuracy of : {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got Test Accuracy of : 87.45454545454545\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, test_dataset_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Classifier is 90.14 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm_train_data = [x.reshape(-1) for x in train_data ]\n",
    "svm_test_data = [x.reshape(-1) for x in test_data]\n",
    "svm = LinearSVC(C=10)\n",
    "svm.fit(svm_train_data, train_label)\n",
    "\n",
    "predictions = svm.predict(svm_test_data)\n",
    "accuracy = sum(np.array(predictions) == test_label) / float(len(test_data))\n",
    "print(\"The accuracy of Classifier is %.2f \" % ((accuracy*100)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "---\n",
    "**Runnable source code in ipynb file and a pdf report are required**.\n",
    "\n",
    "The report should be of 3 to 4 pages describing what you have done and learned in this homework and report performance of your model. If you have tried multiple methods, please compare your results. If you are using any external code, please cite it in your report. Note that this homework is designed to help you explore and get familiar with the techniques. The final grading will be largely based on your prediction accuracy and the different methods you tried (different architectures and parameters).\n",
    "\n",
    "Please indicate clearly in your report what model you have tried, what techniques you applied to improve the performance and report their accuracies. The report should be concise and include the highlights of your efforts."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
